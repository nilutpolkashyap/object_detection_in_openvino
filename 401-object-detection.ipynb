{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from openvino.inference_engine import IECore\n",
    "import depthai as dai\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "\n",
    "import oakd_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "base_model_dir = \"F:/python_workspace/openvino/model\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "model_name = \"ssdlite_mobilenet_v2\"\n",
    "\n",
    "precision = \"FP16\"\n",
    "\n",
    "model_downloader = '\"E:/Program Files (x86)/Intel/openvino_2021.4.752/deployment_tools/open_model_zoo/tools/downloader/downloader.py\"'\n",
    "\n",
    "model_converter = '\"E:/Program Files (x86)/Intel/openvino_2021.4.752/deployment_tools/open_model_zoo/tools/downloader/converter.py\"'\n",
    "\n",
    "# output path for the conversion\n",
    "converted_model_path = f\"F:/python_workspace/openvino/model/public/{model_name}/{precision}/{model_name}.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading ssdlite_mobilenet_v2 ||################\n",
      "\n",
      "========== Downloading F:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2\\ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
      "... 2%, 1024 KB, 840 KB/s, 1 seconds passed\n",
      "... 4%, 2048 KB, 1224 KB/s, 1 seconds passed\n",
      "... 6%, 3072 KB, 1328 KB/s, 2 seconds passed\n",
      "... 8%, 4096 KB, 1456 KB/s, 2 seconds passed\n",
      "... 10%, 5120 KB, 1509 KB/s, 3 seconds passed\n",
      "... 12%, 6144 KB, 1579 KB/s, 3 seconds passed\n",
      "... 14%, 7168 KB, 1555 KB/s, 4 seconds passed\n",
      "... 16%, 8192 KB, 1598 KB/s, 5 seconds passed\n",
      "... 18%, 9216 KB, 1531 KB/s, 6 seconds passed\n",
      "... 20%, 10240 KB, 1575 KB/s, 6 seconds passed\n",
      "... 22%, 11264 KB, 1620 KB/s, 6 seconds passed\n",
      "... 24%, 12288 KB, 1666 KB/s, 7 seconds passed\n",
      "... 26%, 13312 KB, 1714 KB/s, 7 seconds passed\n",
      "... 28%, 14336 KB, 1760 KB/s, 8 seconds passed\n",
      "... 30%, 15360 KB, 1800 KB/s, 8 seconds passed\n",
      "... 32%, 16384 KB, 1830 KB/s, 8 seconds passed\n",
      "... 34%, 17408 KB, 1853 KB/s, 9 seconds passed\n",
      "... 36%, 18432 KB, 1881 KB/s, 9 seconds passed\n",
      "... 39%, 19456 KB, 1912 KB/s, 10 seconds passed\n",
      "... 41%, 20480 KB, 1936 KB/s, 10 seconds passed\n",
      "... 43%, 21504 KB, 1960 KB/s, 10 seconds passed\n",
      "... 45%, 22528 KB, 1983 KB/s, 11 seconds passed\n",
      "... 47%, 23552 KB, 1970 KB/s, 11 seconds passed\n",
      "... 49%, 24576 KB, 1973 KB/s, 12 seconds passed\n",
      "... 51%, 25600 KB, 1988 KB/s, 12 seconds passed\n",
      "... 53%, 26624 KB, 2006 KB/s, 13 seconds passed\n",
      "... 55%, 27648 KB, 2020 KB/s, 13 seconds passed\n",
      "... 57%, 28672 KB, 2036 KB/s, 14 seconds passed\n",
      "... 59%, 29696 KB, 2045 KB/s, 14 seconds passed\n",
      "... 61%, 30720 KB, 2020 KB/s, 15 seconds passed\n",
      "... 63%, 31744 KB, 1985 KB/s, 15 seconds passed\n",
      "... 65%, 32768 KB, 1956 KB/s, 16 seconds passed\n",
      "... 67%, 33792 KB, 1922 KB/s, 17 seconds passed\n",
      "... 69%, 34816 KB, 1893 KB/s, 18 seconds passed\n",
      "... 71%, 35840 KB, 1867 KB/s, 19 seconds passed\n",
      "... 73%, 36864 KB, 1846 KB/s, 19 seconds passed\n",
      "... 76%, 37888 KB, 1836 KB/s, 20 seconds passed\n",
      "... 78%, 38912 KB, 1829 KB/s, 21 seconds passed\n",
      "... 80%, 39936 KB, 1832 KB/s, 21 seconds passed\n",
      "... 82%, 40960 KB, 1834 KB/s, 22 seconds passed\n",
      "... 84%, 41984 KB, 1815 KB/s, 23 seconds passed\n",
      "... 86%, 43008 KB, 1818 KB/s, 23 seconds passed\n",
      "... 88%, 44032 KB, 1821 KB/s, 24 seconds passed\n",
      "... 90%, 45056 KB, 1826 KB/s, 24 seconds passed\n",
      "... 92%, 46080 KB, 1831 KB/s, 25 seconds passed\n",
      "... 94%, 47104 KB, 1838 KB/s, 25 seconds passed\n",
      "... 96%, 48128 KB, 1841 KB/s, 26 seconds passed\n",
      "... 98%, 49152 KB, 1849 KB/s, 26 seconds passed\n",
      "... 100%, 49829 KB, 1847 KB/s, 26 seconds passed\n",
      "\n",
      "========== Unpacking F:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2\\ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python {model_downloader} --name {model_name} --output_dir {base_model_dir} --cache_dir {base_model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Converting ssdlite_mobilenet_v2 to IR (FP16)\n",
      "Conversion command: C:\\ProgramData\\Anaconda3\\python.exe -m mo --framework=tf --data_type=FP16 --output_dir=F:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2\\FP16 --model_name=ssdlite_mobilenet_v2 --reverse_input_channels --input_shape=[1,300,300,3] --input=image_tensor --output=detection_scores,detection_boxes,num_detections \"--transformations_config=E:\\Program Files (x86)\\Intel\\openvino_2021.4.752\\deployment_tools\\model_optimizer/extensions/front/tf/ssd_v2_support.json\" --tensorflow_object_detection_api_pipeline_config=F:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2/ssdlite_mobilenet_v2_coco_2018_05_09/pipeline.config --input_model=F:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb\n",
      "\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tF:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb\n",
      "\t- Path for generated IR: \tF:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2\\FP16\n",
      "\t- IR output name: \tssdlite_mobilenet_v2\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \timage_tensor\n",
      "\t- Output layers: \tdetection_scores,detection_boxes,num_detections\n",
      "\t- Input shapes: \t[1,300,300,3]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tTrue\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tF:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2/ssdlite_mobilenet_v2_coco_2018_05_09/pipeline.config\n",
      "\t- Use the config file: \tNone\n",
      "\t- Inference Engine found in: \tE:\\Program Files (x86)\\Intel\\openvino_2021.4.752\\python\\python3.7\\openvino\n",
      "Inference Engine version: \t2021.4.2-3974-e2a469a3450-releases/2021/4\n",
      "Model Optimizer version: \t2021.4.2-3974-e2a469a3450-releases/2021/4\n",
      "The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: F:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2\\FP16\\ssdlite_mobilenet_v2.xml\n",
      "[ SUCCESS ] BIN file: F:\\python_workspace\\openvino\\model\\public\\ssdlite_mobilenet_v2\\FP16\\ssdlite_mobilenet_v2.bin\n",
      "[ SUCCESS ] Total execution time: 64.24 seconds. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 23:26:36.440605: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n",
      "C:\\Users\\nkele\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "!python {model_converter} --name {model_name} --download_dir {base_model_dir} --precisions {precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inference engine\n",
    "ie_core = IECore()\n",
    "# read the network and corresponding weights from file\n",
    "net = ie_core.read_network(model=converted_model_path)\n",
    "# load the model on the CPU (you can choose manually CPU, GPU, MYRIAD etc.)\n",
    "# or let the engine choose best available device (AUTO)\n",
    "exec_net = ie_core.load_network(network=net, device_name=\"GPU\")\n",
    "\n",
    "# get input and output names of nodes\n",
    "input_key = list(exec_net.input_info)[0]\n",
    "output_key = list(exec_net.outputs.keys())[0]\n",
    "\n",
    "# get input size\n",
    "height, width = exec_net.input_info[input_key].tensor_desc.dims[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('image_tensor', 'DetectionOutput')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_key, output_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "classes = [\n",
    "    \"background\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\",\n",
    "    \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"street sign\", \"stop sign\",\n",
    "    \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
    "    \"bear\", \"zebra\", \"giraffe\", \"hat\", \"backpack\", \"umbrella\", \"shoe\", \"eye glasses\",\n",
    "    \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n",
    "    \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\",\n",
    "    \"plate\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
    "    \"couch\", \"potted plant\", \"bed\", \"mirror\", \"dining table\", \"window\", \"desk\", \"toilet\",\n",
    "    \"door\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\",\n",
    "    \"toaster\", \"sink\", \"refrigerator\", \"blender\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "    \"teddy bear\", \"hair drier\", \"toothbrush\", \"hair brush\"\n",
    "]\n",
    "\n",
    "\n",
    "# colors for above classes (Rainbow Color Map)\n",
    "colors = cv2.applyColorMap(\n",
    "    src=np.arange(0, 255, 255 / len(classes), dtype=np.float32).astype(np.uint8),\n",
    "    colormap=cv2.COLORMAP_RAINBOW\n",
    ").squeeze()\n",
    "\n",
    "\n",
    "def process_results(frame, results, thresh=0.6):\n",
    "    # size of the original frame\n",
    "    h, w = frame.shape[:2]\n",
    "    # results is a tensor [1, 1, 100, 7]\n",
    "    results = results[output_key][0][0]\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for _, label, score, xmin, ymin, xmax, ymax in results:\n",
    "        # create a box with pixels coordinates from the box with normalized coordinates [0,1]\n",
    "        boxes.append(tuple(map(int, (xmin * w, ymin * h, xmax * w, ymax * h))))\n",
    "        labels.append(int(label))\n",
    "        scores.append(float(score))\n",
    "\n",
    "    # apply non-maximum suppression to get rid of many overlapping entities\n",
    "    # see https://paperswithcode.com/method/non-maximum-suppression\n",
    "    # this algorithm returns indices of objects to keep\n",
    "    indices = cv2.dnn.NMSBoxes(bboxes=boxes, scores=scores, score_threshold=thresh, nms_threshold=0.6)\n",
    "\n",
    "    # if there are no boxes\n",
    "    if len(indices) == 0:\n",
    "        return []\n",
    "\n",
    "    # filter detected objects\n",
    "    return [(labels[idx], scores[idx], boxes[idx]) for idx in indices.flatten()]\n",
    "\n",
    "\n",
    "def draw_boxes(frame, boxes):\n",
    "    for label, score, box in boxes:\n",
    "        # choose color for the label\n",
    "        color = tuple(map(int, colors[label]))\n",
    "        # draw box\n",
    "        cv2.rectangle(img=frame, pt1=box[:2], pt2=box[2:], color=color, thickness=3)\n",
    "        # draw label name inside the box\n",
    "        cv2.putText(img=frame, text=f\"{classes[label]}\", org=(box[0] + 10, box[1] + 30),\n",
    "                    fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=frame.shape[1] / 1000, color=color,\n",
    "                    thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main processing function to run object detection\n",
    "def run_object_detection(source=0, flip=False, use_popup=False):\n",
    "    # create video player to play with target fps\n",
    "    player = utils.VideoPlayer(source=source, flip=flip, fps=30)\n",
    "    # start capturing\n",
    "    player.start()\n",
    "    try:\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # grab the frame\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # if frame larger than full HD, reduce size to improve the performance\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(src=frame, dsize=None, fx=scale, fy=scale,\n",
    "                                   interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # resize image and change dims to fit neural network input\n",
    "            input_img = cv2.resize(src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "            # create batch of images (size = 1)\n",
    "            input_img = input_img.transpose(2, 0, 1)[np.newaxis, ...]\n",
    "\n",
    "            # measure processing time\n",
    "            start_time = time.time()\n",
    "            # get results\n",
    "            results = exec_net.infer(inputs={input_key: input_img})\n",
    "            stop_time = time.time()\n",
    "            # get poses from network results\n",
    "            boxes = process_results(frame=frame, results=results)\n",
    "\n",
    "            # draw boxes on a frame\n",
    "            frame = draw_boxes(frame=frame, boxes=boxes)\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # use processing times from last 200 frames\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # mean processing time [ms]\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(img=frame, text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\", org=(20, 40),\n",
    "                        fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=f_width / 1000,\n",
    "                        color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # use this workaround if there is flickering\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # encode numpy array to jpg\n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=frame,\n",
    "                                              params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                # create IPython image\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # display the image in this notebook\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        # stop capturing\n",
    "        player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = \"F:/python_workspace/openvino/bottle-detection.mp4\"\n",
    "\n",
    "run_object_detection(source=video_file, flip=False, use_popup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected cameras:  [<CameraBoardSocket.RGB: 0>, <CameraBoardSocket.LEFT: 1>, <CameraBoardSocket.RIGHT: 2>]\n",
      "Usb speed:  SUPER\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import depthai as dai\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define source and output\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "xoutRgb = pipeline.create(dai.node.XLinkOut)\n",
    "\n",
    "xoutRgb.setStreamName(\"rgb\")\n",
    "\n",
    "# Properties\n",
    "camRgb.setPreviewSize(300, 300)\n",
    "camRgb.setInterleaved(False)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "\n",
    "# Linking\n",
    "camRgb.preview.link(xoutRgb.input)\n",
    "\n",
    "# Connect to device and start pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    print('Connected cameras: ', device.getConnectedCameras())\n",
    "    # Print out usb speed\n",
    "    print('Usb speed: ', device.getUsbSpeed().name)\n",
    "\n",
    "    # Output queue will be used to get the rgb frames from the output defined above\n",
    "    qRgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "\n",
    "    while True:\n",
    "        inRgb = qRgb.get()  # blocking call, will wait until a new data has arrived\n",
    "\n",
    "        # Retrieve 'bgr' (opencv format) frame\n",
    "        cv2.imshow(\"rgb\", inRgb.getCvFrame())\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "            \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dai.Device(pipeline) as device:\n",
    "    qRgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    # main processing function to run object detection\n",
    "    def run_object_detection(source=0, flip=False, use_popup=False):\n",
    "        # create video player to play with target fps\n",
    "        player = utils.VideoPlayer(source=source, flip=flip, fps=30)\n",
    "        # start capturing\n",
    "        player.start()\n",
    "        try:\n",
    "            if use_popup:\n",
    "                title = \"Press ESC to Exit\"\n",
    "                cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "            processing_times = collections.deque()\n",
    "            while True:\n",
    "                # grab the frame\n",
    "#                 frame = player.next()\n",
    "                inRgb = qRgb.get()\n",
    "                frame = inRgb.getCvFrame()\n",
    "                if frame is None:\n",
    "                    print(\"Source ended\")\n",
    "                    break\n",
    "                # if frame larger than full HD, reduce size to improve the performance\n",
    "                scale = 1280 / max(frame.shape)\n",
    "                if scale < 1:\n",
    "                    frame = cv2.resize(src=frame, dsize=None, fx=scale, fy=scale,\n",
    "                                       interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # resize image and change dims to fit neural network input\n",
    "                input_img = cv2.resize(src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "                # create batch of images (size = 1)\n",
    "                input_img = input_img.transpose(2, 0, 1)[np.newaxis, ...]\n",
    "\n",
    "                # measure processing time\n",
    "                start_time = time.time()\n",
    "                # get results\n",
    "                results = exec_net.infer(inputs={input_key: input_img})\n",
    "                stop_time = time.time()\n",
    "                # get poses from network results\n",
    "                boxes = process_results(frame=frame, results=results)\n",
    "\n",
    "                # draw boxes on a frame\n",
    "                frame = draw_boxes(frame=frame, boxes=boxes)\n",
    "\n",
    "                processing_times.append(stop_time - start_time)\n",
    "                # use processing times from last 200 frames\n",
    "                if len(processing_times) > 200:\n",
    "                    processing_times.popleft()\n",
    "\n",
    "                _, f_width = frame.shape[:2]\n",
    "                # mean processing time [ms]\n",
    "                processing_time = np.mean(processing_times) * 1000\n",
    "                fps = 1000 / processing_time\n",
    "                cv2.putText(img=frame, text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\", org=(20, 40),\n",
    "                            fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=f_width / 1000,\n",
    "                            color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "                # use this workaround if there is flickering\n",
    "                if use_popup:\n",
    "                    cv2.imshow(winname=title, mat=frame)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    # escape = 27\n",
    "                    if key == 27:\n",
    "                        break\n",
    "                else:\n",
    "                    # encode numpy array to jpg\n",
    "                    _, encoded_img = cv2.imencode(ext=\".jpg\", img=frame,\n",
    "                                                  params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                    # create IPython image\n",
    "                    i = display.Image(data=encoded_img)\n",
    "                    # display the image in this notebook\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(i)\n",
    "        # ctrl-c\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted\")\n",
    "        # any different error\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            # stop capturing\n",
    "            player.stop()\n",
    "            if use_popup:\n",
    "                cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected cameras:  [<CameraBoardSocket.RGB: 0>, <CameraBoardSocket.LEFT: 1>, <CameraBoardSocket.RIGHT: 2>]\n",
      "Usb speed:  SUPER\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import depthai as dai\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define source and output\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "xoutRgb = pipeline.create(dai.node.XLinkOut)\n",
    "\n",
    "xoutRgb.setStreamName(\"rgb\")\n",
    "\n",
    "# Properties\n",
    "camRgb.setPreviewSize(600, 600)\n",
    "camRgb.setInterleaved(False)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "\n",
    "# Linking\n",
    "camRgb.preview.link(xoutRgb.input)\n",
    "\n",
    "# Connect to device and start pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    print('Connected cameras: ', device.getConnectedCameras())\n",
    "    # Print out usb speed\n",
    "    print('Usb speed: ', device.getUsbSpeed().name)\n",
    "\n",
    "    # Output queue will be used to get the rgb frames from the output defined above\n",
    "    qRgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    processing_times = collections.deque()\n",
    "    title = \"Press 'q' to Exit\"\n",
    "#     cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "        \n",
    "        inRgb = qRgb.get()  # blocking call, will wait until a new data has arrived\n",
    "\n",
    "        frame = inRgb.getCvFrame()\n",
    "        if frame is None:\n",
    "            print(\"Source ended\")\n",
    "            break\n",
    "        # if frame larger than full HD, reduce size to improve the performance\n",
    "#         scale = 1280 / max(frame.shape)\n",
    "#         if scale < 1:\n",
    "#             frame = cv2.resize(src=frame, dsize=None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # resize image and change dims to fit neural network input\n",
    "        input_img = cv2.resize(src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "        # create batch of images (size = 1)\n",
    "        input_img = input_img.transpose(2, 0, 1)[np.newaxis, ...]\n",
    "\n",
    "        # measure processing time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # get results\n",
    "        results = exec_net.infer(inputs={input_key: input_img})\n",
    "        stop_time = time.time()\n",
    "        # get poses from network results\n",
    "        boxes = process_results(frame=frame, results=results)\n",
    "\n",
    "        # draw boxes on a frame\n",
    "        frame = draw_boxes(frame=frame, boxes=boxes)\n",
    "\n",
    "        processing_times.append(stop_time - start_time)\n",
    "        # use processing times from last 200 frames\n",
    "        \n",
    "        if len(processing_times) > 200:\n",
    "            processing_times.popleft()\n",
    "            \n",
    "        _, f_width = frame.shape[:2]\n",
    "        # mean processing time [ms]\n",
    "        processing_time = np.mean(processing_times) * 1000\n",
    "        fps = 1000 / processing_time\n",
    "        \n",
    "        cv2.putText(img=frame, text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\", org=(20, 40),\n",
    "                            fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=f_width / 1000,\n",
    "                            color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "        # Retrieve 'bgr' (opencv format) frame\n",
    "        cv2.imshow(title, frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "            \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvinoenv",
   "language": "python",
   "name": "openvinoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
